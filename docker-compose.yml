version: '3'
services:
  # --- KAFKA & ZOOKEEPER (Standard) ---
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on: [zookeeper]
    ports: ["9092:9092"]
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  # --- AIRFLOW (Merged into One Container) ---
  airflow:
    image: apache/airflow:2.7.1
    ports: ["8080:8080"]
    volumes:
      - ./dags:/opt/airflow/dags
      - ./gdelt_data:/opt/airflow/gdelt_data
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    # This command does EVERYTHING in one place:
    # 1. Installs libraries
    # 2. Initializes DB
    # 3. Creates Admin User
    # 4. Starts Scheduler AND Webserver in background
    command: >
      bash -c "pip install requests kafka-python &&
      airflow db init &&
      airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true &&
      (airflow scheduler & airflow webserver)"

  # --- FLINK ML ENGINE (Your Custom Image) ---
  flink-ml-engine:
    build: .
    image: final-project-env:v1
    container_name: flink_dev_runner
    depends_on: [kafka]
    volumes:
      - ./:/opt/project
    command: tail -f /dev/null